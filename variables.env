#Use latest public image 2.3.0.
VIA_IMAGE=nvcr.io/nvidia/blueprint/vss-engine:2.3.0

#Can update model name eg. "meta/llama-3.1-70b-instruct" 
LLM_MODEL_TO_USE=meta/llama-3.1-70b-instruct
#Can update url to local LLM Instance eg. "http://<ip-addr>:8007/v1" 
BASE_URL_TO_USE=https://integrate.api.nvidia.com/v1

#Adjust ports if needed
FRONTEND_PORT=9100
BACKEND_PORT=8100

#Change default user and pass if needed
GRAPH_DB_USERNAME=neo4j
GRAPH_DB_PASSWORD=password

#Adjust misc configs if needed
DISABLE_GUARDRAILS=true
#For H100 Deployment
NVIDIA_VISIBLE_DEVICES=0
#For L40S Deployment
#NVIDIA_VISIBLE_DEVICES=0,1,2 

#If using NIM(s) locally, specify the local write-access NIM cache
LOCAL_NIM_CACHE=/tmp

### LOCAL_DEPLOYMENT_VLM_SINGLE_GPU: Set VLM to NVILA ###
VLM_MODEL_TO_USE=nvila 
MODEL_PATH=git:https://huggingface.co/Efficient-Large-Model/NVILA-15B
NVILA_VIDEO_MAX_TILES=1

### LOCAL_DEPLOYMENT_VLM: Set VLM to VILA ###
#VLM_MODEL_TO_USE=vila-1.5
#MODEL_PATH=ngc:nim/nvidia/vila-1.5-40b:vila-yi-34b-siglip-stage3_1003_video_v8

### REMOTE_DEPLOYMENT_VLM: Set VLM to OpenAI model ### 
#OPENAI_API_KEY=sk-...
#VLM_MODEL_TO_USE=openai-compat
#VIA_VLM_OPENAI_MODEL_DEPLOYMENT_NAME=gpt-4o
#VIA_VLM_ENDPOINT=https://api.openai.com/v1/chat/completions

### RIVA Settings ### 
#ENABLE_AUDIO=false
#RIVA_ASR_SERVER_URI="grpc.nvcf.nvidia.com"
#RIVA_ASR_GRPC_PORT=443
#RIVA_ASR_SERVER_IS_NIM=true
#RIVA_ASR_SERVER_USE_SSL=true
#RIVA_ASR_SERVER_API_KEY=nvapi-***
#RIVA_ASR_SERVER_FUNC_ID="d8dd4e9b-fbf5-4fb0-9dba-8cf436c8d965"
#DISABLE_CV_PIPELINE=true
